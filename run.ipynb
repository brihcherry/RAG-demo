{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a basic RAG-powered LLM application using Mistral\n",
    "\n",
    "**parse.py**: defines find_documents() and load_documents(), which locates and stores each file in a way that can be used by a RAG system.\n",
    "\n",
    "This notebook loads all the information from files in the chosen directory into a Chroma DB collection. The files are divided according to the default parameters used by the loaders in load_documents. A query is used to extract information from the collection, and the query, context, and some extra information are combined into an LLM prompt that Mistral uses to respond.\n",
    "\n",
    "Areas of improvement: \n",
    "1. change parameters by which documents are split upon loading (done in parse.py load_documents())\n",
    "2. integrate an embedding model when documents are added to the Chroma collection (this file)\n",
    "3. use a pipeline to make the retrieval multi-step or metadata-aware (this file-- but will likely require a lot of code that may end up in other files as well)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import find_documents, load_documents\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.llms import Ollama\n",
    "from embedding_util import CustomEmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DOCUMENTS = 2\n",
    "THRESHOLD = 99999\n",
    "llm = Ollama(model=\"llama2\")\n",
    "TARGET_DIR = 'SOURCE_DIRECTORY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE cell below: produces \"ignoring wrote pointing object at x y (offset z)\" message -- I believe this is coming from the pyPDFLoader functioncall in load_documents. Cause? Is it an issue or can we just leave it there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, filenames = find_documents(TARGET_DIR)\n",
    "documents = load_documents(paths, filenames)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usually takes 10-15 minutes to run when the thermo textbook is being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN\n",
    "try:\n",
    "    #client.reset()\n",
    "    pass\n",
    "finally:\n",
    "    client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "db = client.create_collection(\"newcoll\")\n",
    "\n",
    "db.add(\n",
    "          ids = [str(i) for i in range(0, len(documents))],\n",
    "    documents = [doc.page_content for doc in documents], \n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client:\n",
    "    client.reset()\n",
    "else:\n",
    "    pass\n",
    "\n",
    "client = chromadb.Client(Settings(allow_reset=True))\n",
    "db = client.get_or_create_collection(\n",
    "    name = 'test', embedding_function=CustomEmbeddingFunction()\n",
    ")\n",
    "\n",
    "db.add(\n",
    "    ids = [str(i) for i in range(0, len(documents))],\n",
    "    documents = [doc.page_content for doc in documents], \n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN\n",
    "\n",
    "def create_prompt(context, question):\n",
    "    system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
    "    Read the given context before answering questions and think step by step. If you can not answer a user question based on \n",
    "    the provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\"\n",
    "    B_INST, E_INST = \"<s>[INST] \", \" [/INST]\"\n",
    "    \n",
    "    prompt_template = (\n",
    "    B_INST\n",
    "    + system_prompt\n",
    "    + f\"\"\"\n",
    "            \n",
    "    Context: {context}\n",
    "    User: {question}\"\"\"\n",
    "        + E_INST\n",
    "        + \"\"\"\\n\\nFinally, if any of the context sources supplied to you were useful, list each source like this:\n",
    "                [path to source 1], [path to source 2] etc.\"\"\"\n",
    "            )\n",
    "    #prompt = PromptTemplate(input_variables=['context', 'question'], template=prompt_template)\n",
    "\n",
    "    return (prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This version of the function makes the LLM respond with a narrower answer that is more likely to directly quote the source and list\n",
    "the specific file path and sometimes even section/page number of the source. However, the response is also less robust and doesn't work as\n",
    "well when you give it more than 1 source. The more sources you provide, the less likely it seems to actually cite each source.\n",
    "'''\n",
    "def create_prompt(context, question):\n",
    "    str = f\"\"\"\n",
    "    \n",
    "    You are a helpful assistant that will use some provided context to answer the following question. Before you answer, read the context and think\n",
    "    about how it relates to and answers the question. If you can't answer a question based on the context, simply state that you could not find any useful \n",
    "    information to help answer. Do not use any other information besides the provided context.\n",
    "\n",
    "    {context}\n",
    "    User:{question}\n",
    "\n",
    "    Use this format:\n",
    "    [Filepath] : \n",
    "    [information learned from source]\n",
    "\n",
    "    Thank you!\n",
    "    \"\"\"\n",
    "\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(context, question):\n",
    "    str = f\"\"\"\n",
    "    \n",
    "    You are a helpful assistant that will use some provided context to answer the following question. Before you answer, read the context and think about how it relates to the question. You may be provided with one or several filepaths containing context. If any of the context is relevant, make sure you tell me where it came from-- this may be in the form of a file path, a chapter, or page number. However, if the context is not relevant, prioritize answering the question fully and acknowledge that the provided information was not helpful.\n",
    "\n",
    "    {context}\n",
    "    User:{question}\n",
    "\n",
    "    Please respond by telling me what information you found, where it came from, and then use it to answer the question. \n",
    "\n",
    "    Thank you!\n",
    "    \"\"\"\n",
    "\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recieved_documents(document_list):\n",
    "    #print(f\"LENGTH IS : {len(document_list['metadatas'][0])}\")\n",
    "    #print(f\"LENGTH IS : {len(document_list['documents'][0])}\")\n",
    "    for idx, _ in enumerate(document_list['ids'][0]):\n",
    "        print('************')\n",
    "        print(f\"Filepath: {document_list['metadatas'][0][idx]}\")\n",
    "        #print(f\"distance: {document_list['distances'][0][idx]}\")\n",
    "        print(f\"Content: {document_list['documents'][0][idx]}\")\n",
    "        print('************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to view the prompt\n",
    "#temp = db.query(query_texts='What is the mass of a proton?', n_results=3)\n",
    "#print_recieved_documents(temp)\n",
    "\n",
    "#print(create_prompt(temp, 'What is the mass of a proton?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(question, show_context = True):\n",
    "    results = db.query(query_texts = question, n_results = NUM_DOCUMENTS)\n",
    "    if show_context:\n",
    "        print_recieved_documents(results)\n",
    "    print(\"SELECTED EXCERPTS:\")\n",
    "    for idx, src in enumerate(results['metadatas'][0]):\n",
    "        print(f'    - {src}')\n",
    "    print()\n",
    "\n",
    "    relevant_docs = []\n",
    "\n",
    "    for idx, dist in enumerate(results['distances'][0]):\n",
    "        if dist < THRESHOLD:\n",
    "            this_doc = [ results['metadatas'][0][idx], results['documents'][0][idx]]\n",
    "            relevant_docs.append(this_doc)\n",
    "\n",
    "    response = llm.invoke(create_prompt(relevant_docs, question))\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What is temperature? Can an individual particle have a temperature?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(QUERY)\n",
    "print()\n",
    "print(\"MISTRAL WITH RAG:\")\n",
    "print()\n",
    "get_llm_response(QUERY, show_context = False)\n",
    "#get_llm_response(QUERY)\n",
    "print()\n",
    "print(\"MISTRAL:\")\n",
    "print(llm.invoke(QUERY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
